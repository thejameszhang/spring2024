\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage{epsfig}
\usepackage[tight]{subfigure}
\usepackage{booktabs}
\usepackage{fancyhdr}
\usepackage{hyperref}
\usepackage{graphicx}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=red,
}

% Remember to fill these out.
\newcommand{\StudentName}{James Zhang}
\newcommand{\StudentUID}{118843940}
\newcommand{\PP}{\mathbb{P}}


\newcommand{\myhw}[5]{
  \pagestyle{myheadings}
  \thispagestyle{plain}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to \textwidth { {\bf {\small CMSC 472}:~Introduction to Deep Learning \hfill {\small #2}} }
       \vspace{6mm}
       \hbox to \textwidth { {\Large \hfill #1  \hfill} }
       \vspace{4mm}
      \hbox to \textwidth { {\it Name: {\rm #3} \hfill UID: {\rm #4} } }
      \vspace{2mm}}
      
    }
   \end{center}
   
    \pagestyle{fancy}
    \fancyhf{}
    \lhead{Name: \StudentName}
    \rhead{UID: \StudentUID}
    \cfoot{--- \thepage~---}
    \vspace*{4mm}
}

\parindent 0in
%\parskip 1.5ex
% \renewcommand{\baselinestretch}{1.25}

\begin{document}

\myhw{Assignment 1}{Released: Jan-30. Due Feb-06 5pm.}{\StudentName}{\StudentUID}

\paragraph{Instructions:}
\begin{itemize}
    \item Submit the assignment on Gradescope. You \textbf{must assign} pages to corresponding questions to receive full credit.
    \item Assignments have to be formatted in \LaTeX. You can use \href{https://www.overleaf.com/}{overleaf} for writing your assignments.
    \item Submit only the compiled PDF version of the assignment.
    \item Refer to \href{http://www.cs.umd.edu/class/spring2024/cmsc472/#policy}{policies} (collaboration, late days, etc.) on the course website.
\end{itemize}

\section{Probability}
	
\begin{enumerate}
  \item \textbf{Density function}. Let $p$ be a Gaussian distribution with zero mean and variance of 0.2. Compute the density of $p$ at 0.
  
  \textbf{Sol:}

  Recall that a Gaussian pdf $p$ is defined as 
\[p(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{1}{2}(\frac{x-\mu}{\sigma})^2)\]
Plugging in known values $\mu = 0, \sigma=\sqrt{0.2}$, we get
\[p(x) = \frac{1}{\sqrt{0.4\pi}}\exp(-\frac{1}{2}(\frac{x}{\sqrt{0.2}})^2)\]
Now to find the density at 0, plug in $0$ for $x$ and we get 
\[p(x) = \frac{1}{\sqrt{0.4\pi}} \approx 0.8921 \]
  
  \newpage
  
  \item \textbf{Bayes rule}. Consider the probability distribution of you getting sick given the weather in the table below.
  
  \vspace{1mm}
  \begin{tabular}{@{}ccccc@{}}
  \toprule
  \multirow{2}{*}{Sick?} & \multicolumn{4}{@{}c@{}}{Weather}  \\
  \cmidrule{2-5}
      & sunny & rainy & cloudy & snow \\
  \midrule
  yes  & 0.144 & 0.02 & 0.016 & 0.02\\
  no & 0.576 & 0.08 & 0.064 & 0.08 \\
  \bottomrule
  \end{tabular}
  \vspace{1mm}
  
  Compute P$\left(\, \text{sick}=\text{no}\mid\text{weather}=\text{cloudy} \,\right)$.
  
  \textbf{Sol:}

By the definition of conditional probability
\[\PP(\text{sick=no} \ | \ \text{weather=cloudy}) = \dfrac{\PP(\text{sick=no} \cap \text{weather=cloudy})}{\PP(\text{weather=cloudy})}\]
Looking at the given graph, we plug in 
\[\PP(\text{sick=no} \ | \ \text{weather=cloudy}) = \frac{0.064}{0.08} = 0.8\]
  
  \newpage
  
  \item \textbf{Conditional probability}. A student is taking a one-hour-time-limit makeup examination. Suppose the probability that the student will finish the exam in less than x hours is $x/2$, $\forall \text{x}\in[0,1]$. 
  %for all $0 \leq x \leq 1$. 
  Given that the student is still working after 0.75 hours, what is the conditional probability that the full hour will be used?
  
  \textbf{Sol:}

We're told that $\PP(X < x) = \frac{x}{2} \ \forall \ x \in [0, 1]$. We seek to find
\[\PP(\text{takes exactly 1 hour} \ | \ \text{still working after 0.75 hours})\]
For simplicity, let us denote $A$ as the event the student takes exactly 1 hour, 
and let $B$ denote the event that the student is still working after 0.75 hours.
Once more, by the definition of condiitonal probability, this is equal to
\[\PP(A | B) = \dfrac{\PP(A \cap B)}{\PP(B)}\]
To find $\PP(B)$, we compute
\[\PP(B) = 1 - \frac{0.75}{2} = 0.625\]
For $\PP(A \cap B)$, note that in order for the student to take the full hour, they must still be 
working after $0.75$ hours, so $\PP(A \cap B) = \PP(A) = 1 - \frac{1}{2} = \frac{1}{2}$. 
This stems from the Law of Total Probability, which states 
\[\PP(A) = \sum_n \PP(A \cap B_n)\]
for any events $A, B$. 
Therefore, the answer is 
\[\PP(A | B) = \frac{0.5}{0.625} = 0.8\]
  
  \newpage
\end{enumerate}

\section{Calculus and Linear Algebra}

 For each of the following questions, we expect to see all the steps for reaching the solution.
	
\begin{enumerate}
\item Compute the derivative of the function $f(z)$ with respect to z $\left(\text{i.e., }\frac{df}{dz}\right)$, where $$f(z) = \frac{1}{1+e^{-z}}$$

\textbf{Sol:}

  By the chain rule, we get 
  \[\frac{df}{dz} = -\dfrac{-e^{-z}}{(1 + e^{-z})^2} = \dfrac{e^{-z}}{(1+e^{-z})^2} = \frac{1}{1+e^{-z}}(1 - \frac{1}{1+e^{-z}})\]
  
  \newpage

\item Compute the derivative of the function $f(w)$ with respect to $w_i$, where $w, x \in \mathbb{R}^\text{D}$ and 
\[f(w) = \frac{1}{1+e^{-w^T x}}\]

\textbf{Sol:}

Note that we can rewrite the inner product as a summation so we get 
\[w^T x = -\sum_j w_j x_j, \ j \in [0, D-1] \implies \frac{\partial}{\partial w_i}(-w^T x) = -x_i\]
With respect to a $w_i$,
\[\frac{df}{dw_i} = \dfrac{x_i e^{-w^T x}}{(1+e^{-w^T x})^2}\]
by the chain rule twice.
  
  \newpage

\item Compute the derivative of the loss function $J(w)$ with respect to $w$, where 
\[J(w) = \frac{1}{2}\sum_{i=1}^{m} \left|w^{T}x^{(i)} - y^{(i)}\right|\]

\textbf{Sol:}

  Note that the derivative of a scalar loss function with respect to $w$, a column vector, is a resulting
  row vector. Further note that $\frac{d}{dx}(|x|) = \frac{x}{|x|}$. Thus, with respect to an arbitrary $i$
  \[\frac{d}{dw}(|w^T x^{(i)} - y^{(i)}|) = \dfrac{(w^T x^{(i)} - y^{(i)})x^{(i)^T}}{|w^T x^{(i)} - y^{(i)}|}\]
  
  Therefore, 
  \[\frac{dJ}{dw} = \frac{1}{2}\sum_{i=1}^m  \dfrac{w^T x^{(i)} - y^{(i)}}{|w^T x^{(i)} - y^{(i)}|}(x^{(i)^T})\]
  which is a row vector as desired.

  \newpage

\item Compute the derivative of the loss function $J(w)$ with respect to $w$, where 
\[J(w) = \frac{1}{2}\left[ \sum_{i=1}^{m} \left(w^{T}x^{(i)} - y^{(i)}\right)^2 \right] + \lambda \|w\|^2_2\] 

\textbf{Sol:}
  
Note that this is a loss function with a regression penalty but we can split them up
  \[\frac{dJ}{dw} = \frac{d}{dw}(\frac{1}{2}\sum_{i=1}^m (w^T x^{(i)} - y^{(i)})^2 + \lambda ||w||_2^2) = \frac{1}{2}\frac{d}{dw}(\sum_{i=1}^m (w^T x^{(i)} - y^{(i)})^2) + \frac{d}{dw}(\lambda ||w||^2_2)\]

  Consider an arbitrary $i$, such that we consider
  \[\frac{d}{dw}((w^T x^{(i)} - y^{(i)})^2) = 2(w^Tx^{(i)} - y^{(i)})(x^{(i)T})\]
  Thus, 
  \[\frac{1}{2}\frac{d}{dw}(\sum_{i=1}^m (w^T x^{(i)} - y^{(i)})^2) = \sum_{i=1}^m (w^T x^{(i)} - y^{(i)})(x^{(i)T})\]
  Now for the derivative of the ridge penalty, note that 
  \[\frac{d}{dw}(\lambda ||w||_2^2) = \lambda \frac{d}{dw}(||w||_2^2) = \lambda \frac{d}{dw}(w^T w) = 2\lambda w^T\]
  Therefore,
  \[\frac{dJ}{dw} = 2\lambda w^T + \sum_{i=1}^m (w^T x^{(i)} - y^{(i)})(x^{(i)T})\]

  \newpage

  \item Compute the derivative of the loss function $J(w)$ with respect to $w$, where 
\[J(w) = \sum_{i=1}^{m} \left[y^{(i)} \log \left(\frac{1}{1 + e^{-w^{T}x^{(i)}}}\right) + \left(1 - y^{(i)}\right) \log \left(1- \frac{1}{1 + e^{-w^{T}x^{(i)}}}\right)\right]\]

\textbf{Sol:}

  Recall that we previously calculated that
  \[\frac{d}{dw}(\frac{1}{1 + e^{-w^T x^{(i)}}}) = \frac{e^{-w^T x^{(i)}}x^{(i)^T}}{(1 + e^{-w^T x^{(i)}})^2}\]
  Further note that $\frac{d}{dx}(\log x) = \frac{1}{x}$ plus the chain rule if necessary. Therefore, 
  \[\frac{d}{dw}(\log (\frac{1}{1 + e^{-w^T x^{(i)}}})) = (1 + e^{-w^T x^{(i)}})(\frac{ e^{-w^T x^{(i)}}x^{(i)^T}}{(1 + e^{-w^T x^{(i)}})^2}) = \frac{ e^{-w^T x^{(i)}}x^{(i)^T}}{1 + e^{-w^T x^{(i)}}}\]
  
  Similarly the derivative of $\log(1 - \frac{1}{1 + e^{-w^T x^{(i)}}}) = \log(\frac{e^{-w^T x^{(i)}}}{1 + e^-w^T x^{(i)}})$ to be
  \[\frac{1 + e^{-w^T x^{(i)}}}{e^{-w^T x^{(i)}}} (\frac{ e^{-w^T x^{(i)}}x^{(i)^T}}{(1 + e^{-w^T x^{(i)}})^2}) = -\frac{x^{(i)^T}}{1 + e^{-w^T x^{(i)}}}\]
  which means the derivative over all terms of the summation is 
  \[\frac{dJ}{dw} = \sum_{i=1}^m \left( y^{(i)}\frac{x^{(i)^T} e^{-w^T x^{(i)}}}{1 + e^{-w^T x^{(i)}}} + (1- y^{(i)})\frac{-x^{(i)^T}}{1 + e^{-w^T x^{(i)}}} \right)\]
  \[\frac{dJ}{dw} = \sum_{i=1}^m \left( \frac{y^{(i)}e^{-w^T x^{(i)}} - 1 + y^{(i)}}{1 + e^{-w^Tx^{(i)}}}\right)\left(x^{(i)^T}\right)\]

\newpage

\item Compute $\nabla_w f$, where $f(w) = \tanh \left[w^T x\right]$.

\textbf{Sol:}
\[\tanh(w^T x) = \frac{e^{w^T x} - e^{-w^T x}}{e^{w^T x} + e^{-w^T x}}\]
The gradient is a column vector given as $\begin{bmatrix}
  \frac{\partial f}{\partial x} & \frac{\partial f}{\partial w}
\end{bmatrix}^T$
\[\frac{\partial f}{\partial x} = \dfrac{(e^{w^T x} + e^{-w^T x})(w^Te^{w^T x} + w^Te^{-w^T x}) - (e^{w^Tx} - e^{-w^T x})(w^Te^{w^Tx} - w^Te^{w^T x})}{(e^{w^T x} + e^{-w^T x})^2}\]
\[\frac{\partial f}{\partial x} = w^T(1 - \tanh(w^T x))\]
\[\frac{\partial f}{\partial w} = \dfrac{(e^{x^T x} + e^{-x^T x})(x^Te^{w^T x} + x^Te^{-w^T x}) - (e^{w^Tx} - e^{-w^T x})(x^Te^{w^Tx} - x^Te^{w^T x})}{(e^{w^T x} + e^{-w^T x})^2}\]
\[\frac{\partial f}{\partial w} = x^T(1 - \tanh(w^Tx))\]
Therefore, the gradient is
\[\nabla f = \begin{bmatrix}
  \dfrac{\partial f}{\partial x} \\ \\ \dfrac{\partial f}{\partial w}
\end{bmatrix} = 
  \begin{bmatrix}
  w^T(1 - \tanh(w^Tx))\\ 
  x^T(1- \tanh(w^Tx))
\end{bmatrix}\]

  \newpage
	   
\item Find the solution to the system of linear equations given by Ax=b, where 
\[A = \left(\begin{array}{rrr} 2 & 1 & -1\\ -3 & -1 & 2 \\ -2 & 1 & 2 \end{array}\right) \quad \text{and} \quad b = \left(\begin{array}{r} 8 \\ -11 \\ -3 \end{array}\right)\]

\textbf{Sol:}

  Let row reduce our the appropriate augmented matrix 
  \[\begin{pmatrix}
    2 & 1 & -1 & \vline & 8\\
    -3 & -1 & 2 & \vline & -11\\
    -2 & 1 & 2 & \vline & -3
  \end{pmatrix} \underset{\longrightarrow}{R_2 + \frac{3}{2}R_1}, R_3 + R_1 \begin{pmatrix}
    2 & 1 & -1 & \vline & 8\\
    0 & \frac{1}{2} & \frac{1}{2} & \vline & 1\\
    0 & 2 & 1 & \vline & 5
  \end{pmatrix}\]
  \[\underset{\longrightarrow}{R_1 - 2R_2, R_3 - 4R_2} \begin{pmatrix}
    2 & 0 & -2 & \vline & 6\\
    0 & \frac{1}{2} & \frac{1}{2} & \vline & 1\\
    0 & 0 & -1 & \vline & 1
  \end{pmatrix} \underset{\longrightarrow}{R_1 - 2R_3, R_2 + \frac{1}{2}R_3} \begin{pmatrix}
    2 & 0 & 0 & \vline & 4 \\
    0 & \frac{1}{2} & 0 & \vline & \frac{3}{2}\\
    0 & 0 & -1 & \vline & 1
  \end{pmatrix}\]
  \[ \implies x = \begin{pmatrix}
    2 \\ 3 \\ -1
  \end{pmatrix}\]
  \newpage

\item Find the eigenvalues and associated eigenvectors of the matrix:

\[A = \left[\begin{array}{rrr} 7 & 0 & -3\\ -9 & -2 & 3 \\ 18 & 0 & -8 \end{array}\right] \quad\]

\textbf{Sol:}
  
  To find the eigenvalues of $A$, let us solve $\det(A - \lambda I)$
  \[\det\begin{pmatrix}
    7- \lambda & 0 & -3 \\
    -9 & -2 - \lambda & 3\\
    18 & 0 & -8 - \lambda 
  \end{pmatrix} = (-2-\lambda)((7-\lambda)(-8-\lambda) + 54)\]
  \[ = (-2-\lambda)(\lambda^2 + \lambda - 56 + 54) = (-2-\lambda)(\lambda^2 + \lambda -2) = (-2-\lambda)(\lambda+2)(\lambda-1)\]
  \[\implies \lambda = -2 \text{ with multiplciity 2}, 1\]

  The associated eigenvectors solve the equation $(A - \lambda_i I)v_i = 0$.
  
  For the eigenvalue $-2$, 
  \[\begin{pmatrix}
    9 & 0 & -3\\
    -9 & 0 & 3\\
     18 & 0 & -6
  \end{pmatrix}v = 0 \implies v = \begin{pmatrix}
    0 \\ 1 \\ 0
  \end{pmatrix}, \begin{pmatrix}
    \frac{1}{3} \\ 0 \\ 1
  \end{pmatrix}\]
  For the eigenvalue $1$, 
  \[\begin{pmatrix}
    6 & 0 & -3\\
    -9 & -3 & 3\\
    18 & 0 & -9
  \end{pmatrix}v = 0 \implies \begin{pmatrix}
    2 & 0 & -1\\
    -3 & -1 & 1\\
    0 & 0 & 0\\
  \end{pmatrix}\begin{pmatrix}
    v_1\\v_2\\v_3
  \end{pmatrix} \underset{\longrightarrow}{R_2 + \frac{3}{2}R_1} = \begin{pmatrix}
    2 & 0 & -1\\
    0 & -1 & -\frac{1}{2}\\
    0 & 0 & 0
  \end{pmatrix}v = 0\]
  \[\implies v = \begin{pmatrix}
    1 \\ -1 \\ 2
  \end{pmatrix}\]

  Therefore, the eigenvalue/eigenvalue pairs are 
  \[ \{2, \begin{pmatrix}
    0 \\ 1 \\ 0
  \end{pmatrix}\} , \{2, \begin{pmatrix}
    \frac{1}{3}\\ 0 \\ 1
  \end{pmatrix} \}, \{ 1, \begin{pmatrix}
    1 \\ -1 \\ 2
  \end{pmatrix} \}\]
\end{enumerate}

  \newpage




\section{Activation functions}
For each of the following activation functions, write their equations and their derivatives. Plot the functions and derivatives, with $x\in \left[-5, 5\right]$ and $y\in\left[-10,10\right]$ plot limits. (No need to submit the code for plots.)

\begin{enumerate}
    \item Relu
    
    \textbf{Sol:}

    In all of the following plots, I plot two graphs: the two graphs contains a zoomed in plot
    of the function and its derivative, and the bottom plot contains a zoomed out plot 
    such that $x\in[-5, 5], y \in [-10, 10]$, since I was confused with the directions.
    \[\text{Relu}(x) = \begin{cases}
      0 \text{ if } x < 0\\ 
      x \text{ otherwise }
    \end{cases}\]

    \[\text{Derivative of Relu}(x) = \begin{cases}
      0 \text{ if } x < 0\\
      1 \text{ otherwise } 
    \end{cases}\]

    \[\includegraphics[scale=0.6]{relu.png}\]
    \[\includegraphics[scale=0.6]{relu2.png}\]
  
  \newpage
  
    \item Tanh
    
    \textbf{Sol:}

    \[\text{Tanh}(x) = \dfrac{e^x - e^{-x}}{e^x + e^{-x}}\]

    Using the quotient rule we can determine the derivative
    \[\dfrac{(e^x + e^{-x})(e^x + e^{-x}) - (e^x - e^{-x})(e^x - e^{-x})}{(e^x + e^{-x})^2} = 1 - \frac{(e^x - e^{-x})^2}{(e^x + e^{-x})^2} = 1 - \text{tanh}^2(x)\]
    \[\includegraphics[scale=0.6]{tanh.png}\]
    \[\includegraphics[scale=0.6]{tanh2.png}\]


  \newpage
  
    \item Sigmoid
    
    \textbf{Sol:}

    \[\text{Sigmoid}(x) = \frac{1}{1+e^{-x}}\]
    \[\text{Derivative of Sigmoid}(x) = \frac{e^{-x}}{(1 + e^{-x})^2} = (\frac{1}{1+e^{-x}})(1 - \frac{1}{1+e^{-x}})\]
    
    \[\includegraphics[scale=0.6]{sigmoid.png}\]
    \[\includegraphics[scale=0.6]{sigmoid2.png}\]
  \newpage
  
    \item Leaky ReLU
    
    \textbf{Sol:}

    \[\text{Leaky Relu}(x) = \begin{cases}
      0.01x \text{ if } x < 0\\
      x \text{ otherwise }
    \end{cases}\]

    \[\text{Derivative of Leaky Relu}(x) = \begin{cases}
      0.01 \text{ if } x < 0\\
      1 \text{ otherwise }
    \end{cases}\]
    \[\includegraphics[scale=0.6]{leaky_relu.png}\]
    \[\includegraphics[scale=0.6]{leaky_relu2.png}\]
  \newpage
    \item ELU (plot with $\alpha=0.3$)
    
    \textbf{Sol:}

    \[ELU(x) = \begin{cases}
      \alpha(e^x - 1) \text{ if } x < 0\\ 
      x \text{ otherwise }
    \end{cases}\]

    \[\text{Derivative of ELU(x)} = \begin{cases}
      \alpha e^x \text{ if } x < 0\\
      1 \text{ otherwise }
    \end{cases}\]
    \[\includegraphics[scale=0.6]{elu.png}\]
    \[\includegraphics[scale=0.6]{elu2.png}\]
  
  \newpage
    \item Sinc
    
    \textbf{Sol:}

    \[\text{Sinc}(x) = \frac{\sin(x)}{x}\]
    \[\text{Derivative of Sinc}(x) = \frac{\cos(x)}{x} - \frac{\sin(x)}{x^2}\]
    \[\includegraphics[scale=0.6]{sinc.png}\]
    \[\includegraphics[scale=0.6]{sinc2.png}\]
\end{enumerate}

\vspace{20pt}



\end{document}