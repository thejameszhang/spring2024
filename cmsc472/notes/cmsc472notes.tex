\documentclass[12pt]{scrartcl}
\usepackage[sexy]{james}
\usepackage[noend]{algpseudocode}
\setlength{\marginparwidth}{2cm}
\usepackage{answers}
\usepackage{array}
\usepackage{tikz}
\newenvironment{allintypewriter}{\ttfamily}{\par}
\usepackage{listings}
\usepackage{xcolor}
\usetikzlibrary{arrows.meta}
\usepackage{color}
\usepackage{mathtools}
\newcommand{\U}{\mathcal{U}}
\newcommand{\E}{\mathbb{E}}
\usetikzlibrary{arrows}
\Newassociation{hint}{hintitem}{all-hints}
\renewcommand{\solutionextension}{out}
\renewenvironment{hintitem}[1]{\item[\bfseries #1.]}{}
\renewcommand{\O}{\mathcal{O}}
\declaretheorem[style=thmbluebox,name={Chinese Remainder Theorem}]{CRT}
\renewcommand{\theCRT}{\Alph{CRT}}
\setlength\parindent{0pt}
\usepackage{sansmath}
\usepackage{pgfplots}

\usetikzlibrary{automata}
\usetikzlibrary{positioning}  %                 ...positioning nodes
\usetikzlibrary{arrows}       %                 ...customizing arrows
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\tr}{{\rm tr\ }}
\newcommand{\im}{{\rm Im\ }}
\newcommand{\spann}{{\rm span\ }}
\newcommand{\Col}{{\rm Col\ }}
\newcommand{\Row}{{\rm Row\ }}
\newcommand{\dint}{\displaystyle\int}
\newcommand{\dt}{\ {\rm d }t}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\horizontal}{\par\noindent\rule{\textwidth}{0.4pt}}
\usepackage[top=3cm,left=3cm,right=3cm,bottom=3cm]{geometry}
\newcommand{\mref}[3][red]{\hypersetup{linkcolor=#1}\cref{#2}{#3}\hypersetup{linkcolor=blue}}%<<<changed

\tikzset{node distance=4.5cm, % Minimum distance between two nodes. Change if necessary.
         every state/.style={ % Sets the properties for each state
           semithick,
           fill=cyan!40},
         initial text={},     % No label on start arrow
         double distance=4pt, % Adjust appearance of accept states
         every edge/.style={  % Sets the properties for each transition
         draw,
           ->,>=stealth',     % Makes edges directed with bold arrowheads
           auto,
           semithick}}


% Start of document.
\newcommand{\sep}{\hspace*{.5em}}

\pgfplotsset{compat=1.18}
\begin{document}
\title{CMSC472: Introduction to Deep Learning}
\author{James Zhang\thanks{Email: \mailto{jzhang72@terpmail.umd.edu}}}
\date{\today}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\maketitle
    These are my notes for UMD's CMSC472: Introduction to Deep Learning, 
    which is an elective (``live-\TeX``-ed). This course is taught by Assistant Professor Abhinav Shrivastava. 
\tableofcontents
\newpage

\section{Introduction to Statistical Learning I}

The main idea of statistical learning is to just make sense of data. 

\begin{definition}
  \vocab{Supervised learning}: given inputs (data-label pairs), learn a model to predict output.
  In mathematical terms, we want to learn a prediction function $f$. 
\end{definition}

\begin{definition}
  \vocab{Training (or learning)}: given a training set of labeled examples $\{(x_1, y_1), 
  \cdots, (x_N, y_N)\}$, instantiate a predictor$f$. $y = f(x) \in \mathcal{H}$, this is known as the \vocab{hypothesis space}.
\end{definition}

\begin{definition}
  \vocab{Testing (or inference)}: apply $f$ to a new test example and return the output $f(x)$. Note that we measure correctness
  using loss functions.
\end{definition}

\begin{note}
  Training and testing data should be i.i.d.\ (independent and identically distributed) samples from the same distribution $D$.
\end{note}

\subsection{Simple Models: Classification and Regression}

\begin{definition}
  A \vocab{nearest neighbor classifer} returns $f(x) = $ the label of the training example nearest to $x$. 
  All we need is a distance function. Note that this model requires no training. 

  \hfill

  Similarly, a \vocab{K-Nearest Neighbor} finds $k$ nearest points and $f(x) = $ vote for class labels
  with labels of the $k$ nearest points. Note that kNN is more robust to outliers.
\end{definition}

\begin{itemize}
  \item kNN pros: Simple to implement, decision boundaries not necessary linear, 
works for any number of classes, nonparametric method
  \item kNN cons: need good distance function, slow at test time
\end{itemize}

\begin{definition}
  A \vocab{linear classifier} is a linear function that separates the classes such that 
  $f(x) = \text{sgn}(\textbf{w} \cdot \textbf{x} + b)$.
\end{definition}

\begin{note}
  Perceptrons cannot do nonlinearly separated data. 
\end{note}

\begin{itemize}
  \item Linear pros: low-dimensional parametric representation, easy to learn, very fast at test time
  \item Linear cons: works for two classes(?), how to train the linear function, data is not linearly separable
\end{itemize}


\end{document}

