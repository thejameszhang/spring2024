\documentclass[12pt]{scrartcl}
\usepackage[sexy]{james}
\usepackage[noend]{algpseudocode}
\setlength{\marginparwidth}{2cm}
\usepackage{answers}
\usepackage{array}
\usepackage{tikz}
\newenvironment{allintypewriter}{\ttfamily}{\par}
\usepackage{listings}
\usepackage{xcolor}
\usetikzlibrary{arrows.meta}
\usepackage{color}
\usepackage{mathtools}
\newcommand{\U}{\mathcal{U}}
\newcommand{\E}{\mathbb{E}}
\usetikzlibrary{arrows}
\Newassociation{hint}{hintitem}{all-hints}
\renewcommand{\solutionextension}{out}
\renewenvironment{hintitem}[1]{\item[\bfseries #1.]}{}
\renewcommand{\O}{\mathcal{O}}
\declaretheorem[style=thmbluebox,name={Chinese Remainder Theorem}]{CRT}
\renewcommand{\theCRT}{\Alph{CRT}}
\setlength\parindent{0pt}
\usepackage{sansmath}
\usepackage{pgfplots}

\usetikzlibrary{automata}
\usetikzlibrary{positioning}  %                 ...positioning nodes
\usetikzlibrary{arrows}       %                 ...customizing arrows
\newcommand{\eqdef}{=\vcentcolon}
\newcommand{\tr}{{\rm tr\ }}
\newcommand{\im}{{\rm Im\ }}
\newcommand{\spann}{{\rm span\ }}
\newcommand{\Col}{{\rm Col\ }}
\newcommand{\Row}{{\rm Row\ }}
\newcommand{\dint}{\displaystyle\int}
\newcommand{\dt}{\ {\rm d }t}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\horizontal}{\par\noindent\rule{\textwidth}{0.4pt}}
\usepackage[top=3cm,left=3cm,right=3cm,bottom=3cm]{geometry}
\newcommand{\mref}[3][red]{\hypersetup{linkcolor=#1}\cref{#2}{#3}\hypersetup{linkcolor=blue}}%<<<changed

\tikzset{node distance=4.5cm, % Minimum distance between two nodes. Change if necessary.
         every state/.style={ % Sets the properties for each state
           semithick,
           fill=cyan!40},
         initial text={},     % No label on start arrow
         double distance=4pt, % Adjust appearance of accept states
         every edge/.style={  % Sets the properties for each transition
         draw,
           ->,>=stealth',     % Makes edges directed with bold arrowheads
           auto,
           semithick}}


% Start of document.
\newcommand{\sep}{\hspace*{.5em}}

\pgfplotsset{compat=1.18}
\begin{document}
\title{CMSC472: Introduction to Deep Learning}
\author{James Zhang\thanks{Email: \mailto{jzhang72@terpmail.umd.edu}}}
\date{\today}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\maketitle
    These are my notes for UMD's CMSC472: Introduction to Deep Learning, 
    which is an elective (``live-\TeX``-ed). This course is taught by Assistant Professor Abhinav Shrivastava. 
\tableofcontents
\newpage

\section{Introduction to Statistical Learning I}

The main idea of statistical learning is to just make sense of data. 

\begin{definition}
  \vocab{Supervised learning}: given inputs (data-label pairs), learn a model to predict output.
  In mathematical terms, we want to learn a prediction function $f$. 
\end{definition}

\begin{definition}
  \vocab{Training (or learning)}: given a training set of labeled examples $\{(x_1, y_1), 
  \cdots, (x_N, y_N)\}$, instantiate a predictor$f$. $y = f(x) \in \mathcal{H}$, this is known as the \vocab{hypothesis space}.
\end{definition}

\begin{definition}
  \vocab{Testing (or inference)}: apply $f$ to a new test example and return the output $f(x)$. Note that we measure correctness
  using loss functions.
\end{definition}

\begin{note}
  Training and testing data should be i.i.d.\ (independent and identically distributed) samples from the same distribution $D$.
\end{note}

\subsection{Simple Models: Classification and Regression}

\begin{definition}
  A \vocab{nearest neighbor classifer} returns $f(x) = $ the label of the training example nearest to $x$. 
  All we need is a distance function. Note that this model requires no training. 

  \hfill

  Similarly, a \vocab{K-Nearest Neighbor} finds $k$ nearest points and $f(x) = $ vote for class labels
  with labels of the $k$ nearest points. Note that kNN is more robust to outliers.
\end{definition}

\begin{itemize}
  \item kNN pros: Simple to implement, decision boundaries not necessary linear, 
works for any number of classes, nonparametric method
  \item kNN cons: need good distance function, slow at test time
\end{itemize}

\begin{definition}
  A \vocab{linear classifier} is a linear function that separates the classes such that 
  $f(x) = \text{sgn}(\textbf{w} \cdot \textbf{x} + b)$.
\end{definition}

\begin{note}
  Perceptrons cannot do nonlinearly separated data. 
\end{note}

\begin{itemize}
  \item Linear pros: low-dimensional parametric representation, easy to learn, very fast at test time
  \item Linear cons: works for two classes (?), how to train the linear function, data is not linearly separable
\end{itemize}

\subsection*{Unsuprvised Learning}

\begin{definition}
  
  Given just data as input (no labels), \vocab{unsupervised learning} models
  learn some sort of underlying structure. 
\end{definition}

\begin{itemize}
  \item Clustering
  \item Quantization
  \item Dimensionality reduction, also called manifold learning. 
  \item Density estimation, which can be used for anomoly detection. This idea is also used
  for training GANs.
\end{itemize}

\begin{definition}
  \vocab{Weakly-Supervised Learning} is where all data needs to be labeled, but the labels
  are noisy (incomplete or wrong). 
\end{definition}

\begin{definition}
  \vocab{Semi-supervised learning} needs nice clean labels, but there's a small
  portion of data that needs to labeled. If you know the underlying structure of the data,
  then it's fine. 
\end{definition}

\begin{definition}
  \vocab{Bootstrapping} is the process of improving labelling accuracy in a semi-supervised learning sense. 
  Bootstrapping is often used in building ML datasets.
\end{definition}

\begin{definition}
  In \vocab{active learning}, there's a human in the loop to ask questions and labels. 
\end{definition}

\begin{definition}
  \vocab{Self-supervised learning / Predictive learning} uses proxy-tasks to turn
  an unsupervised learning problem into a supervised learning problem. It is 
  a machine learning process where the model trains itself to learn one part of the input 
  from another part of the input. It is also known as predictive or pretext learning. 
  This differs from unsupervised learning because self-supervised learning is more 
  focused on the data than the model. 
\end{definition}

\begin{definition}
  \vocab{Proxy-tasks}, such as image colorization, use supervision naturally arising from data.
  No human provided labels. Completes a task whose labels will be useful for a supervised learning model. 
\end{definition}

\begin{definition}
  \vocab{Reinforcement Learning} is the process of an agent learning in an interactive environment
  by sequential trial and error using feedback/rewards from its own actions and experiences.
\end{definition}

\section{Introduction to Neural Networks I}

\begin{definition}
  A \vocab{neural network} is a 'differentiable' function composed out of multiple layers of computation.
\end{definition}

\begin{definition}
  A \vocab{tensor} is a d-dimensional array.
  \begin{itemize}
    \item Vector: 1-d tensor
    \item Matrix: 2-d tensor
    \item Tensor: n-d tensor
  \end{itemize}
  Tensors are inputs and outputs of layers. 
\end{definition}

\begin{note}
  A single neuron is, very simply, one linear regression task, $\hat{y} = \textbf{w}^T \textbf{x} + b$.
  If $x$ is an n-dimensional vector, and $m$ is an m-dimensional vector, then 
  $w$ is $n\times m$ matrix and $b$ is an m-dimensional vector, and this gives us one neuron.

  \hfill

  Now consider three neurons stacked on top of each other. This is a fully-connected layer. 
  Take the previous layer of output, and use it as an input to the next layer.
  For example,
  \[\hat{y} = w_2^T(w_1^T x + b_1) + b_2 = w_2^T(w_1^T x) + w_2^T b_1 + b_2\]
  You can just collapse this (distributive property).
\end{note}

\begin{note}
  We need an $f$ to prevent collapsing
  \[\hat{y} = w_2^T f(w_1^T x + b_1) + b_2\]
  and these $f$'s give the neural networks nonlinearities. These $f$'s are 
  \vocab{activation functions}.
\end{note}

\begin{definition}
  \vocab{Sigmoid} is $f(x) = \sigma(x) = \frac{1}{1 + e^{-x}}$ This function squashes
  everything between 0 and 1. 
\end{definition}

\begin{definition}

  \vocab{Tanh} is $f(x) = \text{tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$. This function
  squashes everything between -0.5 and 0.5.
\end{definition}

\begin{definition}
  \vocab{Relu} is $f(x) = \max(0, x)$ which is self-explanatory.
\end{definition}




\end{document}

